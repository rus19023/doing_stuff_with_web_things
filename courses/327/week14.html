<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link type="text/css" rel="stylesheet" href="https://rus19023.github.io/myportfolio/css/styles.css" />
        <title id="cit327weekpagetitle" class="title">page title</title>
    </head>

    <body>
        <header id="autoheader327" class="title">auto header</header>
        <main>
            <h1 id="cit327weekpagetitle2" class="padit title">page title</h1>
            <h2 id="paperTitle" class="padit title">paper title</h2>
            <div>

              <h3 class="title">Objective</h3>
              <p class="normal-text">
                  Write a 5-10 position page paper, which in addition to the requirements below, focuses on maximizing telemetry and reducing technical debt.
                  <br><br>
                  The goal of this final project is to perform an analysis of the data warehouse to support CIO-level decision-making.
                  <br><br>
                  The idea is that you will be looking at the parts of the data warehouse model below (not including those boxes that are not filled in with blue).
                  <figure>
                      <img class="centerimg" src="../../images/Model.jpg" alt="Model graphic given to help with capstone project for CIT327">
                      <figcaption class="source">Model graphic given to help with capstone project for CIT327</figcaption>
                  </figure>
              </p>

              <h2 class="title"> Requirements</h2>
              <ul>
                  <li class="normal-text">
                      Explain what methods, tools, and techniques are best suited to deliver a manageable corporate data warehouse [or data mart?]
                  </li>
                  <li class="normal-text">
                      Explain when you should use Inmon or Kimball methods for your data marts and warehouses.
                  </li>
                  <li class="normal-text">
                      Provide perspective (agree, disagree or provided alternative) with the statement that “embedded structures, tables, and documents like JSON, are big data elements, and almost always require ETL processes to simplify their structures”.
                  </li>
                  <li class="normal-text">
                      Define how would you go about selecting an analytical tool for the corporation; consider analytical programming but avoid where possible (due to high technical debt) when other tool sets can met needs effectively at low cost (like MicroStrategy, Tableau, Microsoft Power BI, etc.)
                  </li>
                  <li class="normal-text">
                      Explain how you would qualify big data

                      "embedded structures, tables, and documents like JSON, are big data elements"
                  </li>
                  <li class="normal-text">
                      Qualify how the selected seven aspects of data analysis (identified further below) work [with] various data sources, specifically
                      <ul>
                        <li class="normal-text">
                            a. Relational or object relational database management systems.
                        </li>
                        <li class="normal-text">
                            b. NoSQL (Not only SQL) database management systems, like Cassandra and MongoDB.
                        </li>
                        <li class="normal-text">
                            c. XML raw files and database managed documents.
                        </li>
                      </ul>
                  </li>
                  <li class="normal-text">

                  </li>
              </ul>

                <h2 class="title">Definitions</h2>

                <h3 class="title">Analytical tool:</h3>
                In context of this paper, the assumption is that “analytical tool” refers to the end-to-end analytics platform, specifically:
                <ul>
                    <li class="normal-text">
                        Database layer for data storage and processing
                    </li>
                    <li class="normal-text">
                        ETL/ELT for data acquisition and integration
                    </li>
                    <li class="normal-text">
                        Presentation layer for user access, visualization and analytics
                    </li>
                </ul>

                <h3 class="title">IT telemetry: </h3>
                <p class="normal-text">
                    “the automatic recording and transmission of data from remote or inaccessible sources to an IT system in a different location for monitoring and analysis” (source).
                    <br>
                    Technical debt: “the extra [future] development work that arises when code that is easy to implement in the short run is used instead of applying the best overall solution.” (source)
                </p>


                <h2 class="title">7 Aspects of Data Analysis</h2>

                <h3 class="title">Data Visualization:</h3>
                <p class="normal-text">
                    “Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data. In the world of Big Data, data visualization tools and technologies are essential to analyze massive amounts of information and make data-driven decisions.” (source)
                </p>

                <h3 class="title">Analytics:</h3>
                <p class="normal-text">
                    “The field of data analysis. Analytics often involves studying past historical data to research potential trends, to analyze the effects of certain decisions or events, or to evaluate the performance of a given tool or scenario. The goal of analytics is to improve the business by gaining knowledge which can be used to make improvements or changes.” (source)
                </p>


                <h3 class="title">Online Analytical Processing (OLAP):</h3>
                <p class="normal-text">
                    “...enables end-users to perform ad hoc analysis of data in multiple dimensions, thereby providing the insight and understanding they need for better decision making [by providing the] ability to create very fast aggregations and calculations of underlying data sets.” (source) · Document Management: “a document-oriented database contains documents, which are records that describe the data in the document, as well as the actual data; one or more document [can be used] to represent a real-world object.” (source)
                </p>

                <h3 class="title">Decision Services:</h3>
                    see note
                <p class="normal-text">

                </p>
                <p class="normal-text">
                    Integrations: “To integrate data across mixed application environments, you need to get data from one data environment (source) to another data environment (destination).
                </p>

                <h3 class="title"></h3>
                <p class="normal-text">
                    Extract, Transform and Load (ETL) technologies have been used to accomplish this in traditional data warehouse environments.” (source)
                </p>

                <h3 class="title"></h3>
                <p class="normal-text">
                    Big Data Integration: “...differs from traditional data integration in many dimensions: Volume, Velocity, Variety and Veracity, which are the big data main characteristics” (source)
                </p>

        Do we have the data in the data warehouse If not, where does that data exist, is it in a relational database is it a no sequel database is it in unstructured data in files.
        21:24
        If once we identify that then the next thing we have to figure out is what do we need to do with that data so that we can get it into the data warehouse so that we can.
        21:32
        So that we can answer the business question that has come up that's the incremental portion, but when you first start out.
        21:41
        you're usually you you're usually told we want to reporting program we want to do, reporting and you've got to go out and build out the base architecture.
        21:52
        knowing what your environment is and say okay well how am I going to how am I going to integrate that information.
        21:58
        What type of data warehouse am I going to have what type of reporting, am I going to have that's going to minimize tech debt but maximize telemetry and and you're going to start with your first question through that that process once it's built.

        What you're going to do is you're going to want to compare inman versus.
        23:34
        Inmon versus Kimball.
        23:36
        So, knowing that kimball is your star scheme ID normalized inman is normalized and when, would you use either one.
        23:47
        that's going to be your your first decision and you're going to have to take a look and say what is the how do I, how do I pull the data from my source systems into that.
        23:59
        Now, when when you're looking at choosing your analytical tool if let's say that you're talking you make the assumption that this is a small business.
        24:10
        You know a lot of times and I don't I can't remember who who argued for in men and who argued.
        24:16
        kimball in the first assignment, but a lot of the people that argued for kimball said, you know we're making the assumption that this is a smaller business just getting started out.
        24:26
        Well, if it's a smaller business just getting started out and they don't want to do the full inman model and they just don't want to do data marts with a specific.
        24:34
        Area of data, like accounting data or HR data they're probably not going to get into these advanced analytics and so as they as they select an analytics tool.
        24:44
        Reporting and business intelligence, are going to be probably their primary goal, so, if you look, if you want to if you're saying my business is a smaller business.
        24:53
        you're probably going to pick the data Mart with a specific subject area you're probably going to pick a off the shelf reporting tool you're probably going to pick some type of an ETF tool that that.
        25:06
        Can kind of read relational databases and maybe no sequel databases, but you might not be you might not need stream processing, you might not need.
        25:16
        hadoop and mapreduce because your databases your operational databases might not be horizontally scale you probably have a single instance relational database and maybe a single instance of Cassandra or of.
        25:34
        mongo and so you're not talking about these huge architectures that have multiple nodes of multiple and multiple instances now if you say my business is big.
        25:47
        And i'm trying to ingest all of my subject areas that might require that you take the approach of i'm going to do admin.
        25:55
        i'm not only going to have a business intelligence tool but I needed to do a little bit more and you're going to want maybe tablo can get you into some data mining or some forecasting.
        26:06
        or power bi.
        26:08
        Or you might say, i'm going to use tableau for my reporting and business intelligence, but i'm going to do more analytic programming with our or SAS to get into these some of these other areas.
        26:21
        In that case, you might have a much more complex operational environment.
        26:26
        You might have multiple relational databases, you might have a mongo database like family search where there's probably hundreds of nodes.
        26:36
        And you have to to be able to pull data from all different places and you might need stream processing.
        26:43
        And so, with a bigger business, you might not be able to do with just a kimball and a simple reporting database, you might have to use hadoop and an inman model and and not only a reporting instance, but you might have to use some analytic.
        26:59
        Programming to make that all work so depending on depending on what you make as the assumption of the business that you're representing is really going to determine which path you're going to get to to this analytic tool recommendation for your CIO.

        the Google blog it's a big table this is for fast reads and writes.
        35:46
        For heavy reads and writes.
        35:48
        Big query is large scale ad hoc sequel based on lap analysis.
        35:54
        So yeah once you put it in here, and you need a lab analysis that's where big query Is this because it's a column store it's fast reads and writes.
        user avatar
        cai
        36:05
        And that is similar so, then you have like redshift right that's similar to big query but then you have big table that's similar to like EC to.

        kUBERNETES

        container management tool it's it's a scripting tool for your container management, it allows you to automate the process of creating containers like PC to configuring those and deploying knows.


        <h1 class="title">DELETE UP TO HERE     ///\\\   DUE APRIL 7, 2022   ///\\\</h1>
        <br><br><br>


        <h2 class="title">Create and Maintain a Manageable Corporate Data Warehouse</h2>
        <h3 class="title">Introduction</h3>
        
          <p class="normal-text">
              This paper is to share concepts I have learned about Big Data Analysis and Warehousing, and the accompanying Technology, Methodology and the available Tools and Techniques.
          </p>
          <p class="normal-text">
              With the massive numbers of available tools, frameworks, libraries, databases, companies and corporations, developers, techniques and other resources available today, that is increasing at rocket speed, managing Big Data is a colossal issue.
          </p>
          <h3 class="title">Big Data</h3>
          <p class="normal-text">
              Many data relationships can be represented by a hierarchical or tree structure. In the Enterprise Database Server, these relationships can be represented by including a data set among the items of a record. If a data set contains another data set as an item, then the contained data set is called an embedded data set, and the data record in which it is declared is called the owner or master of the embedded structure. Any number of embedded records can belong to each master.
            Sets, subsets, and accesses can be embedded within a data set.

              From <https://public.support.unisys.com/aseries/docs/clearpath-mcp-18.0/86000213-420/section-000019644.html>
          </p>
          <p class="normal-text">
            One popular interpretation of big data refers to extremely large data sets. A National Institute of Standards and Technology report defined big data as consisting of “extensive datasets—primarily in the characteristics of volume, velocity, and/or variability—that require a scalable architecture for efficient storage, manipulation, and analysis.” Some have defined big data as an amount of data that exceeds a petabyte—one million gigabytes.

            From <https://datasciencedegree.wisconsin.edu/data-science/what-is-big-data/> 
          </p>

        <h3 class="title">Data Models</h3>
            2. Section 2 – Project Setup
            <p class="normal-text">
                Which model to use when: Inmon or Kimball?

            </p>
            <p class="normal-text">

            </p>

            <h3 class="title">Data Transformation</h3>
            What are the methods and tools and techniques that i'm going to be utilizing that's going to be your transformation stage.

            How does that fit into my big data elements of my of my operational systems from there that's going to be the that's going to be the PRIMARY PORTION of your paper and I and again

            I would say that you said that goes right along with this so kimball or inman what methods or techniques, do you agree that you, you need this ETF layer
            16:38
            What would you use for your analytics.
            16:42
            And then, finally, how does it fit in with those other these other.
            16:47
            pieces of data visualization analytics all lab document management decision services integrations all of these are really just covered in what we talked about.

            
                
            <h4 class="title">Requirements and Constraints</h4>
                a. Identify project drivers and desired outcomes (the “why?” and maybe “when?”)
                c. Document requirements and constraints (the “what?”)
                <p class="normal-text">
                  "The data requirements analysis process employs a top-down approach that emphasizes business-driven needs, so the analysis is conducted to ensure the identified requirements are relevant and feasible. The process incorporates data discovery and assessment in the context of explicitly qualified business data consumer needs. Having identified the data requirements, candidate data sources are determined and their quality is assessed using the data quality assessment process described in chapter 11."

                  From <https://www.sciencedirect.com/topics/computer-science/data-requirement-analysis>

              </p>
                <h4 class="title">Data Quality</h4>
              
              <p class="normal-text">
                "Data quality is crucial – it assesses whether information can serve its purpose in a particular context (such as data analysis, for example). So, how do you determine the quality of a given set of information? There are data quality characteristics of which you should be aware. 
                 There are five traits that you’ll find within data quality: accuracy, completeness, reliability, relevance, and timeliness – read on to learn more.
                   • Accuracy
                   • Completeness
                   • Reliability
                   • Relevance
                   • Timeliness"

               From <https://www.precisely.com/blog/data-quality/5-characteristics-of-data-quality> 
             </p>
             
             <h4 class="title">Data end-to-end analytics platform</h4>
              <p class="normal-text">

              </p>

            <h4 class="title">Data Analysis</h4>
              b. Establish the participants – stakeholders, technology team, etc. (the “who?”)
              <p class="normal-text">
                  "A data analyst takes the raw data and analyzes it to draw out useful insights. They then present these insights in the form of visualizations, such as graphs and charts, so that stakeholders can understand and act upon them. The kinds of insights gleaned from the data depends on the type of analysis performed. There are four main types of analysis used by data experts: Descriptive, diagnostic, predictive, and prescriptive. Descriptive analytics looks at what happened in the past, while diagnostic analytics looks at why it might have happened. Predictive and prescriptive analytics consider what is likely to happen in the future and, based on these predictions, what the best course of action might be."

                  From <https://careerfoundry.com/en/blog/data-analytics/> 
              </p>
              <h3 class="title">Data Visualization</h3>
              <p class="normal-text col3">
                  <img src="../images/pivot.png" alt="pivot table image" class="fifty centered">
                  <img src="../images/piechart.png" alt="pivot table image" class="fifty centered">
                  <img src="../images/boxplot.png" alt="pivot table image" class="fifty centered">
                  <img src="../images/linegraph.png" alt="pivot table image" class="fifty centered">
                  <img src="../images/areachart.png" alt="pivot table image" class="fifty centered">
                  <img src="../images/barchart.png" alt="pivot table image" class="fifty centered">
                  Images from <a href="https://careerfoundry.com/en/blog/data-analytics/data-visualization-types/">Career Foundry online.</a>
              </p>
              <p class="normal-text">
                  "Data visualization (or ‘data viz’) is one of the most important aspects of data analytics. Mapping raw data using graphical elements is great for aiding pattern-spotting and it’s useful for sharing findings in an easily digestible, eye-catching way. And while the priority should always be the integrity of your data, if done well, data visualization can also be a lot of fun.
                  Master the art of data viz and you’ll soon be spotting trends and correlations, all while flexing your creative muscle."


                  https://docs.google.com/presentation/d/e/2PACX-1vQS_n-iAUCKBmTuE8oMwD5cNRCVL55eS9Wh15lF8z8S6cgKx1-W4wZ3rRxE_6WgwNZtKwJlEH_eU-KA/pub?start=false&loop=false&delayms=30000

                  From <https://careerfoundry.com/en/blog/data-analytics/data-visualization-types/>
              </p>
              <p class="normal-text">

              </p>

          
              <p class="normal-text">

              </p>

          <h4 class="title">Methodology</h4>
            d. Choose the methodology - Inmon, Kimball, hybrid (the “how?”)
            <p class="normal-text">

            </p>
            <p class="normal-text">

            </p>

        <h3 class="title">Project Evaluation</h3>
            3. Section 3 - Project Execution
            <p class="normal-text">
                Combining Streaming Telemetry with Other Data Sources
                From the network operations perspective, streaming telemetry can improve efficiency in many use cases, including:
                • Detecting problems by setting up network monitors and alerts based on pre-configured thresholds or network performance baselines
                • Troubleshooting connectivity and performance issues
                • Planning for network capacity according to usage and budgets
                • And much more… especially when we are able to use AI or machine-learning techniques to make automated decisions based on telemetry data.
                However, streaming telemetry shouldn’t be the only data source that drives these capabilities. As an example:
                Let’s say that you, as a network operator, want to be notified when utilization is high for critical backbone links. The next step would be to determine the characteristics of the traffic that’s driving up utilization. Which applications, clients, and servers are prominent on the highly-utilized links and can thus be used to make vaious optimization decisions (e.g., changing traffic patterns)?
                An appropriate approach could be:
                  1. Use streaming telemetry metrics as a set of indicators of thresholds, and then
                  2. Use NetFlow to figure out what type of traffic is causing it.
                
                From <https://www.kentik.com/blog/how-to-maximize-the-value-of-streaming-telemetry-for-network-monitoring-and/> 
            </p>
          <p class="normal-text">
              Technical debt is a term that conceptualizes the tradeoff between the short-term benefits of rapid delivery and the long-term value of developing a software system that is easy to evolve, modify, repair, and sustain. Like financial debt, technical debt can be a burden or an investment. Technical debt can be a burden when it is taken on unintentionally without a solid plan to manage it. Technical debt can also be part of an intentional investment strategy that speeds up development … as long as you have a plan to pay it back before the interest swamps your principal.  
              From <https://www.sei.cmu.edu/our-work/projects/display.cfm?customel_datapageid_4050=6520>
          </p>
          <p class="normal-text">
            When it comes to software development, technical debt is the idea that certain necessary work gets delayed during the development of a software project in order to hit a deliverable or deadline.

            Technical debt is the coding you must do tomorrow because you took a shortcut in order to deliver the software today. <https://www.bmc.com/blogs/technical-debt-explained-the-complete-guide-to-understanding-and-dealing-with-technical-debt/>
          </p>

      <h4 class="title">Working Processes</h4>
              a. Define working processes (project management, prioritization, bug triage, tech debt 
            management, etc.)
            <p class="normal-text">
              A critical product insight is understanding user engagement (who is using the product, how often, what flows, etc.). Typically, a data pipeline is built to aggregate the user clickstream details, scripts/programs in spark, hive, impala, etc. to cleanse, process and extract the engagement metrics. The pipeline can be running daily, hourly, or in real-time.

              Now, multiply the number of such pipelines by 1000s given the different types of business dashboards, ML models, data products that a typical enterprise runs today. How do you manage and optimize the performance, SLA, cost, resource allocation, quality of these pipelines processing data at petabyte scale? This is a non-trivial problem given the plethora of technology building blocks involved in a data pipeline and the deep expertise required to efficiently map pipeline configurations to workload and application requirements.

              Enter Data Science for Big Data Telemetry. By analyzing billions of data points aggregated across hardware-, cluster-, job-, and application-level, the goal is to provide data users and engineers an understanding into: 1) What is the current state; 2) Why root-causes and signals; 3) How-to tune the application, cluster, resource allocation to get to an optimal state. Data pipelines today are treated no longer as an IT overhead, but as part of the business differentiator powering the insights, data products, AI/ML explorations within the enterprise. Data Science for Big Data Telemetry is critical in answering questions such as:
                  • How much budget do I need for running my pipelines based on workload patterns (especially in the cloud)?
                  • Why is this spark application running slow?
                  • How do I improve the resource utilization of the big data cluster?
                  • Is my CPU-memory allocation within the cluster balanced?
                  • Is a particular big data application rogue?
                  • Which cloud instance types should I migrate when moving from on-premise to cloud?
                  • Given the optimization goal (such as cost, performance), finding the best configuration to run the big data pipeline?

                From <https://www.linkedin.com/pulse/unraveling-data-science-big-telemetry-join-mission-sandeep/> 

            </p>
            <p class="normal-text">
                "The majority of companies today realize the value of a data-driven business strategy and are in need of talented individuals to provide insight into the constant stream of collected information. Research shows that nearly 70 percent of U.S. executives say they will prefer job candidates with data skills by 2021, and the demand for analysts will only grow as we continue to digitize our physical world.
                If you’re just starting your research and are wondering how to make the transition to a career in data analytics, you’re not alone. Scanning job postings for data-driven positions is a great starting point, but many analyst roles are highly nuanced, making it difficult to discern which skills are the most necessary to invest in.""

                From <https://www.northeastern.edu/graduate/blog/data-analyst-skills/> 
            </p>
            <p class="normal-text">
                But first, let’s define telemetry and what it means for gaming companies. Defined as an automated communications process of recording and receiving data, telemetry is utilized in wide range of industries. In the gaming industry, telemetry is a fundamental part of game analytics. In fact, it’s the core of the game analytics realm in a lot of ways.
                What’s more, the definition of telemetry can be simplified even more as the ability to collect multiple variables and data sets over time.
                Gaming telemetry or recording and receiving data on user behavior covers a lot of activities – in fact, everything from the naming conventions and structure of the data itself to the operations performed on it could be covered under that umbrella term. However, it’s often also used to describe the raw data itself as it exists in analytics software or reports. And of course, the goal of all of this is improving performance and solving problems in game development, marketing, and research.

                From <https://gameanalytics.com/blog/maximizing-the-value-of-player-data/#telemetry-in-the-context-of-gaming> 
            </p>

            <h4 class="title">Technology</h4>
                b. Select technologies/platforms and tools
                <p class="normal-text">
                    "The App development industry is the most competitive and rewarding industry of this decade. Business owners that want to start a new business or want to expand their existing business are making app development their first and foremost priority.
                    App development is easier and simpler with MEAN stack and Full-stack developers. However, due to the increasing demand for app development, getting hold of a developer’s expert in MEAN technology and Full-stack is harder and expensive."

                    From <https://www.softsuave.com/blog/mean-stack-vs-full-stack-developer-hire-for-your-next-project/> 
                </p>
                <p class="normal-text">

                    telemetry’s primary benefit is found in a combination of the data it gleans and how that data is used to generate insights.

                    OpenTelemetry is a cloud-native, vendor-neutral collection of application programming interfaces (APIs), tools, and software development kits (SDKs). It’s used to generate telemetry data, as well as collect and export it. You can then use what OpenTelemetry produces to gain insights into the way your software performs or behaves.
                    OpenTelemetry supports a variety of languages, including:
                    • Java
                    • C#
                    • C++
                    • JavaScript
                    • Python
                    • Rust
                    • Erlang/Elixir
                    OpenTelemetry also integrates with many common libraries and frameworks, including:
                    • MySQL
                    • Django
                    • Redis
                    • Kafka
                    • Jetty
                    • RabbitMQ
                    • Akka
                    • Spring
                    • Flask
                    • gorilla/mux
                    • net/http
                    • WSGI
                    • JDBC
                    • PostgreSQL
                    With OpenTelemetry, you can forward telemetric data from your software and services to analysis tools. This allows you to gather, organize, and analyze telemetry data to gain insights.

                    From <https://www.stackstate.com/blog/the-ultimate-guide-to-telemetry> 
                </p>

            <h4 class="title">Developing, Testing, Releasing</h4>
                c. Begin developing, testing, releasing
                <p class="normal-text">
                    "Identifying and dealing with outliers can be tough, but it is an essential part of the data analytics process, as well as for feature engineering for machine learning. So how do we find outliers?"

                    From <https://careerfoundry.com/en/blog/data-analytics/how-to-find-outliers/>
                </p>
                <p class="normal-text">

                </p>

            <h3 class="title">Managing Telemetry and Technical Debt</h3>

            <h4 class="title">Telemetry</h4>
                " Measure project over time to ensure continued success"
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>



            <h4 class="title">Technical Debt</h4>
                <p class="normal-text">
                    Technical debt is the name given to previously unforeseen issues that cause extra financial cost, including salaries for developers.
                </p>
                <p class="normal-text">
                    There are many reasons why technical debt exists. 
                </p>
                <ul>
                    <li class="normal-text">

                    </li>
                    <li class="normal-text">
                        
                    </li>
                    <li class="normal-text">
                        
                    </li>
                    <li class="normal-text">
                        
                    </li>
                </ul>
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>
                "Reasons of Technical Debts
                There are two main reasons: Unwanted debts due to lacks of skill or knowledge and consciously accepted ones e.g. due to time pressure. But there are also many technical reasons for both categories [1]:
                    • Architectural principles are not adhered to, such as deviating from the layer model
                    • Poor technical infrastructure
                    • Lack of development standards and design patterns
                    • Inaccurate business requirements that are misunderstood and implemented
                    • Insufficient testing which results in unwanted errors
                    • Constantly changing or additional requirements, etc…"

                <https://towardsdatascience.com/understand-and-fight-technical-debts-in-your-data-warehouse-d455afed770e></https:>
                <p class="normal-text">
                    "The Cloud Native Computing Foundation (CNCF) maintains an interactive graphic of the nearly 1,000 unique services that make up the cloud-native ecosystem, many of which are free and open source to boot. Additionally, each of the big three cloud providers—Amazon Web Services, Microsoft Azure, and Google Cloud—offers about 200 unique services to customers, across compute, storage, database, analytics, networking, mobile, developer tools, management tools, IoT, security, and enterprise applications.

                    “The process of application development is simply too fragmented at this point; the days of every enterprise architecture being three-tier, every database being relational, and every business application being written in Java and deployed to an application server are over,” wrote RedMonk analyst Stephen O’Grady in a 2020 blog post. “The single most defining characteristic of today’s infrastructure is that there is no single defining characteristic. It’s diverse to a fault.”  <https://www.infoworld.com/article/3639050/complexity-is-killing-software-developers.html>



                </p>
                <p class="normal-text">

                </p>

                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>

            <h4 class="title">Ongoing Maintenance and Enhancement</h4>
                Establish maintenance process for ongoing sustainment and enhancement
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>

            <h3 class="title">Conclusion</h3>
                Section 5 – Conclusion
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>

            <h3 class="title"></h3>
                <p class="normal-text">

                </p>
                <p class="normal-text">

                </p>

                <p class="normal-text">
                    <br />
                    <figure>
                        <img src="../../images/" alt="alt text here" class="centerimg" />
                        <br><br>
                        <figcaption class="source">
                            text here. &nbsp; <br>
                            Source:
                            <a href="">link text here </a>
                        </figcaption>
                    </figure>
                </p>

                <ul class="sources">
                    Sources:
                    <li class="normal-text">
                        <a class="source" href="https://public.support.unisys.com/aseries/docs/clearpath-mcp-18.0/86000213-420/section-000019644.html">
                            [] "Enterprise Database Server Data and Structure Definition Language (DASDL) Programming Reference Manual - Embedded Structures", public.support.unisys.com Line 151
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://datasciencedegree.wisconsin.edu/data-science/what-is-big-data">
                            [] "What Is Big Data?", University of Wisconsin, Data Science  Line 156
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.sciencedirect.com/topics/computer-science/data-requirement-analysis">
                            [] Loshin, David "Data Requirement Analysis", Science Direct Line 173
                        </a>
                    </li>  
                    <li class="normal-text">
                        <a class="source" href="https://careerfoundry.com/en/blog/data-analytics/data-visualization-types">
                            [] Hiller, Will  "13 of the Most Common Types of Data Visualization", Career Foundry, 26 July 2021  line 180
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://careerfoundry.com/en/blog/data-analytics">
                            [] "What is data analytics?", Career Foundry Line 200
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.precisely.com/blog/data-quality/5-characteristics-of-data-quality">
                            [] Line 217
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.kentik.com/blog/how-to-maximize-the-value-of-streaming-telemetry-for-network-monitoring-and">
                            [] line 247
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.sei.cmu.edu/our-work/projects/display.cfm?customel_datapageid_4050=6520">
                            [] line 251
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.bmc.com/blogs/technical-debt-explained-the-complete-guide-to-understanding-and-dealing-with-technical-debt">
                            line 257
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.linkedin.com/pulse/unraveling-data-science-big-telemetry-join-mission-sandeep">
                            [] line 275
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.northeastern.edu/graduate/blog/data-analyst-skills">
                            line 282
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://gameanalytics.com/blog/maximizing-the-value-of-player-data/#telemetry-in-the-context-of-gaming">
                            [] line 289
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://www.softsuave.com/blog/mean-stack-vs-full-stack-developer-hire-for-your-next-project">
                            [] line 298
                        </a>
                    </li>
                    <li class="normal-text">E
                        <a class="source" href="https://www.stackstate.com/blog/the-ultimate-guide-to-telemetry">
                            [] line 331
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://careerfoundry.com/en/blog/data-analytics/how-to-find-outliers">
                            [] line 338
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://towardsdatascience.com/understand-and-fight-technical-debts-in-your-data-warehouse-d455afed770e">
                            [] Lauer, Christian, "9 Technical Debts in your Data Warehouse - Reasons and how to solve them", Towards Data Science, 26 December 2020
                        </a>
                    </li>
                    <li class="normal-text">
                        <a href="https://www.infoworld.com/author/Scott-Carey/">
                            [] Carey, Scott
                        </a>,
                        <a class="source" href="https://www.infoworld.com/article/3639050/complexity-is-killing-software-developers.html">
                            "Complexity is killing software developers", InfoWorld, 1 November 2021
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="https://en.wikipedia.org/wiki/Big_data">
                            [] "Big Data", wikipedia.com, last edited 1 April 2022
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="">
                            [] linktext
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="">
                            [] linktext
                        </a>
                    </li>
                  <li class="normal-text">
                      <a class="source" href="">
                        [] linktext
                      </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="">
                            [] linktext
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="">
                            [] linktext
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="">
                            [] linktext
                        </a>
                    </li>
                    <li class="normal-text">
                        <a class="source" href="">
                            [] linktext
                        </a>
                    </li>
                </ul>
            </div>
        </main>
        <footer id="autofooter"></footer>
        <script src="../../js/main.js"></script>
        <script src="../../js/menu.js"></script>
    </body>
</html>
